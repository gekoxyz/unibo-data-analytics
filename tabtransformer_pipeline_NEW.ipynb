{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edc2a42",
   "metadata": {},
   "source": [
    "# Strategy for KNN\n",
    "\n",
    "for sure i will need to randomly undersample and SMOTE oversample the training set (TRAIN ONLY) to keep balance\n",
    "\n",
    "Use correlation between features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32640029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Replaces sklearn Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from tab_transformer_pytorch import TabTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88640189",
   "metadata": {},
   "source": [
    "# load the dataset and fix values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a6eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device is available.\n",
      "Shape of the dataset: (148301, 145)\n",
      "Number of duplicates in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS device is available.\")\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"CUDA device is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No GPU acceleration available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Fix the seed to have deterministic behaviour\n",
    "def fix_random(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "SEED = 1337\n",
    "fix_random(SEED)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "DATASET_PATH = \"dataset_train/dataset.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH, delimiter=\",\")\n",
    "\n",
    "print(f\"Shape of the dataset: {dataset.shape}\")\n",
    "duplicates = dataset[dataset.duplicated()]\n",
    "print(f\"Number of duplicates in the dataset: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae26d9",
   "metadata": {},
   "source": [
    "## split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe853ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=[\"grade\"])\n",
    "y = dataset[\"grade\"].map({\"A\": 6, \"B\": 5, \"C\": 4, \"D\": 3, \"E\": 2, \"F\": 1, \"G\": 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a61d55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts integers from strings using regex\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "        return X\n",
    "\n",
    "class CyclicalDateEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Converts mm-yyyy to year + sine/cosine month encoding.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            # errors=\"coerce\" turns unparseable data/NaNs into NaT\n",
    "            date_series = pd.to_datetime(X[col], format=\"%b-%Y\", errors=\"coerce\")\n",
    "            # If date is NaT, these become NaN, which we handle in the pipeline later\n",
    "            angle = 2 * np.pi * date_series.dt.month / 12\n",
    "\n",
    "            X[f\"{col}_year\"] = date_series.dt.year\n",
    "            X[f\"{col}_month_sin\"] = np.sin(angle)\n",
    "            X[f\"{col}_month_cos\"] = np.cos(angle)\n",
    "            \n",
    "            X.drop(columns=[col], inplace=True)\n",
    "        return X\n",
    "    \n",
    "class BinaryModeEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\"Encodes 0 if value is mode, 1 if not\"\"\"\n",
    "    def __init__(self):\n",
    "        self.modes_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate mode for each column and store it\n",
    "        for col in X.columns:\n",
    "            self.modes_[col] = X[col].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col, mode in self.modes_.items():\n",
    "            # Apply: 1 if NOT the mode (least frequent), 0 if mode\n",
    "            X_copy[col] = (X_copy[col] != mode).astype(int)\n",
    "        return X_copy\n",
    "    \n",
    "class HighMissingDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns with high missing percentage. Fits only on training data.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=20):\n",
    "        self.threshold = threshold\n",
    "        self.cols_to_drop_ = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate missing percentages only on training data\n",
    "        missing_percentages = X.isna().mean() * 100\n",
    "        self.cols_to_drop_ = missing_percentages[missing_percentages > self.threshold].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.cols_to_drop_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "821899ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = ['loan_title']\n",
    "binary_cols = [\"loan_payment_plan_flag\", \"listing_initial_status\", \"application_type_label\",\n",
    "               \"hardship_flag_indicator\", \"disbursement_method_type\", \"debt_settlement_flag_indicator\"]\n",
    "one_hot_encoding_cols = [\"borrower_housing_ownership_status\", \"borrower_income_verification_status\",\n",
    "                       \"loan_status_current_code\", \"loan_purpose_category\", \"borrower_address_state\"]\n",
    "extract_fields = [\"loan_contract_term_months\", \"borrower_profile_employment_length\", \"borrower_address_zip\"]\n",
    "date_fields = [\"loan_issue_date\", \"credit_history_earliest_line\", \"last_payment_date\", \"last_credit_pull_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597c54be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118640, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# --- 1. Define Encoders ---\n",
    "\n",
    "# Numeric: Impute -> Scale (Move Scaling HERE)\n",
    "numeric_pipe = SkPipeline([\n",
    "    ('extract', NumericExtractor()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()) \n",
    "])\n",
    "\n",
    "# Categorical/Date/Binary: Impute -> Ordinal Encode (Integers)\n",
    "# We use handle_unknown='use_encoded_value' with unknown_value=-1 to handle new categories in test\n",
    "# We will shift these +1 later so unknown becomes 0\n",
    "ord_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=np.float32)\n",
    "\n",
    "categorical_pipe = SkPipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ordinal', ord_encoder) \n",
    "])\n",
    "\n",
    "# Re-using the same logic for dates/binary to keep them as integer embeddings\n",
    "# (Assuming dates are feature-engineered or low cardinality. If raw dates, this is risky)\n",
    "date_pipe = SkPipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ordinal', ord_encoder)\n",
    "])\n",
    "\n",
    "binary_pipe = SkPipeline([\n",
    "    ('binary_enc', BinaryModeEncoder()), # Ensure this outputs identifiable categories\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', ord_encoder)\n",
    "])\n",
    "\n",
    "# --- 2. Build Preprocessor ---\n",
    "# CRITICAL: We group all categorical outputs FIRST in the transformers list.\n",
    "# This ensures the first N columns of our output matrix are categories, and the rest are continuous.\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # --- GROUP A: CATEGORICALS ---\n",
    "        ('cat_pipe', categorical_pipe, one_hot_encoding_cols),\n",
    "        ('date_pipe', date_pipe, date_fields),\n",
    "        ('bin_pipe', binary_pipe, binary_cols),\n",
    "        \n",
    "        # --- GROUP B: CONTINUOUS ---\n",
    "        ('num_pipe', numeric_pipe, extract_fields),\n",
    "        \n",
    "        # Drop redundant\n",
    "        ('drop_redundant', 'drop', redundant_cols)\n",
    "    ],\n",
    "    # Drop anything else (or passthrough if you are sure)\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# Main Pipeline (Removed Global Scaler)\n",
    "pipeline = SkPipeline([\n",
    "    ('dropper', HighMissingDropper(threshold=20)),\n",
    "    ('prep', preprocessor),\n",
    "])\n",
    "\n",
    "X_clean_train = pipeline.fit_transform(X_train, y_train)\n",
    "X_clean_test = pipeline.transform(X_test)\n",
    "\n",
    "print(X_clean_train.shape)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_clean_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_clean_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor),\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07b7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: (8, 5, 11, 16, 53, 140, 657, 136, 120, 3, 3, 3, 3, 3, 3)\n",
      "Split: 15 Categorical, 3 Continuous\n"
     ]
    }
   ],
   "source": [
    "# --- Step A: Get Vocab Sizes for the Model ---\n",
    "def get_vocab_sizes(pipe_name):\n",
    "    # Access: Pipeline -> ColumnTransformer -> Sub-Pipeline -> OrdinalEncoder\n",
    "    enc = pipeline.named_steps['prep'].named_transformers_[pipe_name].named_steps['ordinal']\n",
    "    # Add +1 to every count to reserve index 0 for \"unknown\"\n",
    "    return [len(cats) + 1 for cats in enc.categories_]\n",
    "\n",
    "# Collect vocab sizes\n",
    "cat_vocab  = get_vocab_sizes('cat_pipe')\n",
    "date_vocab = get_vocab_sizes('date_pipe')\n",
    "bin_vocab  = get_vocab_sizes('bin_pipe')\n",
    "\n",
    "categories_tuple = tuple(cat_vocab + date_vocab + bin_vocab)\n",
    "num_cat_cols = len(categories_tuple)\n",
    "num_cont_cols = X_clean_train.shape[1] - num_cat_cols\n",
    "dim_out = len(np.unique(y_train)) \n",
    "\n",
    "print(f\"Categories: {categories_tuple}\")\n",
    "print(f\"Split: {num_cat_cols} Categorical, {num_cont_cols} Continuous\")\n",
    "\n",
    "# --- Step B: Helper to Split and Shift Data ---\n",
    "def prepare_tensors(X_data, y_data):\n",
    "    # Split the matrix\n",
    "    X_cat_part = X_data[:, :num_cat_cols]\n",
    "    X_cont_part = X_data[:, num_cat_cols:]\n",
    "    \n",
    "    # 1. Categorical: Cast to Long and Shift +1 (Unknown -1 becomes 0)\n",
    "    x_categ = torch.tensor(X_cat_part, dtype=torch.long) + 1 \n",
    "    \n",
    "    # 2. Continuous: Cast to Float\n",
    "    x_cont = torch.tensor(X_cont_part, dtype=torch.float32)\n",
    "    \n",
    "    # 3. Labels: Cast to Long\n",
    "    y = torch.tensor(y_data.values, dtype=torch.long) \n",
    "    \n",
    "    return x_categ, x_cont, y\n",
    "\n",
    "# --- Step C: Create Loaders ---\n",
    "# Train\n",
    "x_train_cat, x_train_cont, y_train_t = prepare_tensors(X_clean_train, y_train)\n",
    "train_dataset = TensorDataset(x_train_cat, x_train_cont, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Test\n",
    "x_test_cat, x_test_cont, y_test_t = prepare_tensors(X_clean_test, y_test)\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(x_test_cat, x_test_cont, y_test_t),\n",
    "    batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbc8f038",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m batch_y = batch_y.to(device)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Forward pass: Pass TWO inputs (Cat, Cont)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_cont\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Loss calculation\u001b[39;00m\n\u001b[32m     41\u001b[39m loss = criterion(outputs, batch_y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/tab_transformer_pytorch/tab_transformer_pytorch.py:228\u001b[39m, in \u001b[36mTabTransformer.forward\u001b[39m\u001b[34m(self, x_categ, x_cont, return_attn)\u001b[39m\n\u001b[32m    225\u001b[39m     shared_categ_embed = repeat(\u001b[38;5;28mself\u001b[39m.shared_category_embed, \u001b[33m'\u001b[39m\u001b[33mn d -> b n d\u001b[39m\u001b[33m'\u001b[39m, b = categ_embed.shape[\u001b[32m0\u001b[39m])\n\u001b[32m    226\u001b[39m     categ_embed = torch.cat((categ_embed, shared_categ_embed), dim = -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m x, attns = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcateg_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_attn\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m flat_categ = rearrange(x, \u001b[33m'\u001b[39m\u001b[33mb ... -> b (...)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    231\u001b[39m xs.append(flat_categ)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/tab_transformer_pytorch/tab_transformer_pytorch.py:117\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, return_attn)\u001b[39m\n\u001b[32m    114\u001b[39m     x, post_softmax_attn = attn(x)\n\u001b[32m    115\u001b[39m     post_softmax_attns.append(post_softmax_attn)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     x = \u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m x = \u001b[38;5;28mself\u001b[39m.reduce_streams(x)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_attn:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/hyper_connections/manifold_constrained_hyper_connections.py:495\u001b[39m, in \u001b[36mManifoldConstrainedHyperConnections.forward\u001b[39m\u001b[34m(self, residuals, *branch_args, **branch_kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    489\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    490\u001b[39m     residuals,\n\u001b[32m    491\u001b[39m     *branch_args,\n\u001b[32m    492\u001b[39m     **branch_kwargs\n\u001b[32m    493\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     branch_input, residuals, residual_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwidth_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_residual_fn\u001b[39m(branch_out):\n\u001b[32m    499\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add_branch_out_to_residual:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/hyper_connections/manifold_constrained_hyper_connections.py:381\u001b[39m, in \u001b[36mManifoldConstrainedHyperConnections.width_connection\u001b[39m\u001b[34m(self, residuals)\u001b[39m\n\u001b[32m    377\u001b[39m alpha_pre, alpha_residual = alpha[..., :\u001b[38;5;28mself\u001b[39m.num_input_views], alpha[..., \u001b[38;5;28mself\u001b[39m.num_input_views:]\n\u001b[32m    379\u001b[39m alpha_pre = alpha_pre.sigmoid()\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m alpha_residual = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresidual_mix_constraint_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha_residual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m alpha = cat((alpha_pre, alpha_residual), dim = -\u001b[32m1\u001b[39m)\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_dynamic_alpha_proposals:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/hyper_connections/manifold_constrained_hyper_connections.py:59\u001b[39m, in \u001b[36msinkhorn_knopps\u001b[39m\u001b[34m(log_alpha, iters)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[32m     58\u001b[39m     alpha = l1norm(alpha, dim = -\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     alpha = \u001b[43ml1norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m alpha.to(dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/hyper_connections/manifold_constrained_hyper_connections.py:47\u001b[39m, in \u001b[36ml1norm\u001b[39m\u001b[34m(t, dim)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34ml1norm\u001b[39m(t, dim):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/nn/functional.py:5571\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(input, p, dim, eps, out)\u001b[39m\n\u001b[32m   5567\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   5568\u001b[39m         normalize, (\u001b[38;5;28minput\u001b[39m, out), \u001b[38;5;28minput\u001b[39m, p=p, dim=dim, eps=eps, out=out\n\u001b[32m   5569\u001b[39m     )\n\u001b[32m   5570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5571\u001b[39m     denom = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.clamp_min(eps).expand_as(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   5572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m / denom\n\u001b[32m   5573\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/_tensor.py:871\u001b[39m, in \u001b[36mTensor.norm\u001b[39m\u001b[34m(self, p, dim, keepdim, dtype)\u001b[39m\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    868\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    869\u001b[39m         Tensor.norm, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, p=p, dim=dim, keepdim=keepdim, dtype=dtype\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/functional.py:1812\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(input, p, dim, keepdim, out, dtype)\u001b[39m\n\u001b[32m   1810\u001b[39m _p = \u001b[32m2.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m p\n\u001b[32m   1811\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.linalg.vector_norm(\n\u001b[32m   1815\u001b[39m         \u001b[38;5;28minput\u001b[39m, _p, _dim, keepdim, dtype=dtype, out=out\n\u001b[32m   1816\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "# --- Step D: Instantiate Model ---\n",
    "model = TabTransformer(\n",
    "    categories = categories_tuple,\n",
    "    num_continuous = num_cont_cols,\n",
    "    dim = 32,\n",
    "    dim_out = dim_out,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    attn_dropout = 0.1,\n",
    "    ff_dropout = 0.1,\n",
    "    mlp_hidden_mults = (4, 2),\n",
    "    mlp_act = nn.ReLU()\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# --- Step E: Training Loop ---\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # CRITICAL: Loop unpacks 3 items now\n",
    "    for batch_cat, batch_cont, batch_y in train_loader:\n",
    "        batch_cat = batch_cat.to(device)\n",
    "        batch_cont = batch_cont.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass: Pass TWO inputs (Cat, Cont)\n",
    "        outputs = model(batch_cat, batch_cont)\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f50c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9174\n",
      "Balanced Accuracy: 0.8825\n",
      "F1 score: 0.9169\n"
     ]
    }
   ],
   "source": [
    "# 1. Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# 2. Disable gradient calculation for memory efficiency\n",
    "with torch.no_grad():\n",
    "    # Move test data to the same device as the model (CPU or GPU)\n",
    "    X_test_tensor = X_test_tensor.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_test_tensor)\n",
    "    \n",
    "    # 3. Get predicted classes\n",
    "    # For CrossEntropy (7 classes), we take the index of the highest logit\n",
    "    _, y_pred_tensor = torch.max(outputs, 1)\n",
    "    \n",
    "    # 4. Convert to NumPy for Scikit-Learn\n",
    "    # Ensure it's on CPU before converting to numpy\n",
    "    y_pred = y_pred_tensor.cpu().numpy()\n",
    "    \n",
    "# 5. Calculate and print metrics\n",
    "# Note: y_test should be your original labels or y_test_tensor.cpu().numpy()\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "bacc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"Balanced Accuracy: {bacc:.4f}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f\"F1 score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
