{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb68583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from pytorch_tabular.models import TabNetModelConfig\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig\n",
    "from pytorch_tabular import TabularModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a9e0e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device is available.\n",
      "Shape of the dataset: (148301, 145)\n",
      "Number of duplicates in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS device is available.\")\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"CUDA device is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No GPU acceleration available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Fix the seed to have deterministic behaviour\n",
    "def fix_random(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "SEED = 1337\n",
    "fix_random(SEED)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "DATASET_PATH = \"dataset_train/dataset.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH, delimiter=\",\")\n",
    "\n",
    "print(f\"Shape of the dataset: {dataset.shape}\")\n",
    "duplicates = dataset[dataset.duplicated()]\n",
    "print(f\"Number of duplicates in the dataset: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=[\"grade\"])\n",
    "y = dataset[\"grade\"].map({\"A\": 6, \"B\": 5, \"C\": 4, \"D\": 3, \"E\": 2, \"F\": 1, \"G\": 0})\n",
    "\n",
    "# 1. First Split: Separate out the final Hold-out Test set (e.g., 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Second Split: Separate Train from Validation (e.g., 10% of total, or 12.5% of the temp data)\n",
    "# This ensures Validation data is never seen by the \"fit\" method\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.125, stratify=y_temp\n",
    ")\n",
    "# Resulting Ratios roughly: Train (70%), Val (10%), Test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8daf6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts integers from strings using regex\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "        return X\n",
    "    \n",
    "    def set_output(self, *, transform=None):\n",
    "        # We ignore the 'transform' argument because this class \n",
    "        # always returns a DataFrame (pandas) by design.\n",
    "        return self\n",
    "\n",
    "class CyclicalDateEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Converts mm-yyyy to year + sine/cosine month encoding.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            # errors=\"coerce\" turns unparseable data/NaNs into NaT\n",
    "            date_series = pd.to_datetime(X[col], format=\"%b-%Y\", errors=\"coerce\")\n",
    "            # If date is NaT, these become NaN, which we handle in the pipeline later\n",
    "            angle = 2 * np.pi * date_series.dt.month / 12\n",
    "\n",
    "            X[f\"{col}_year\"] = date_series.dt.year\n",
    "            X[f\"{col}_month_sin\"] = np.sin(angle)\n",
    "            X[f\"{col}_month_cos\"] = np.cos(angle)\n",
    "            \n",
    "            X.drop(columns=[col], inplace=True)\n",
    "        return X\n",
    "    \n",
    "    def set_output(self, *, transform=None):\n",
    "        # We ignore the 'transform' argument because this class \n",
    "        # always returns a DataFrame (pandas) by design.\n",
    "        return self\n",
    "    \n",
    "class BinaryModeEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\"Encodes 0 if value is mode, 1 if not\"\"\"\n",
    "    def __init__(self):\n",
    "        self.modes_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate mode for each column and store it\n",
    "        for col in X.columns:\n",
    "            self.modes_[col] = X[col].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col, mode in self.modes_.items():\n",
    "            # Apply: 1 if NOT the mode (least frequent), 0 if mode\n",
    "            X_copy[col] = (X_copy[col] != mode).astype(int)\n",
    "        return X_copy\n",
    "    \n",
    "    def set_output(self, *, transform=None):\n",
    "        # We ignore the 'transform' argument because this class \n",
    "        # always returns a DataFrame (pandas) by design.\n",
    "        return self\n",
    "    \n",
    "class HighMissingDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns with high missing percentage. Fits only on training data.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=20):\n",
    "        self.threshold = threshold\n",
    "        self.cols_to_drop_ = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        missing_percentages = X.isna().mean() * 100\n",
    "        self.cols_to_drop_ = missing_percentages[missing_percentages > self.threshold].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.cols_to_drop_)\n",
    "    \n",
    "    def set_output(self, *, transform=None):\n",
    "        # We ignore the 'transform' argument because this class \n",
    "        # always returns a DataFrame (pandas) by design.\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6d581d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = [\"loan_title\", \"borrower_address_state\"]\n",
    "\n",
    "binary_cols = [\"loan_payment_plan_flag\", \"listing_initial_status\", \"application_type_label\",\n",
    "               \"hardship_flag_indicator\", \"disbursement_method_type\", \"debt_settlement_flag_indicator\"]\n",
    "extract_fields = [\"loan_contract_term_months\", \"borrower_profile_employment_length\"]\n",
    "date_fields = [\"loan_issue_date\", \"credit_history_earliest_line\", \"last_payment_date\", \"last_credit_pull_date\"]\n",
    "\n",
    "# instead of one hot + embedding we just embed\n",
    "one_hot_encoding_cols = [\"borrower_housing_ownership_status\", \"borrower_income_verification_status\",\n",
    "                       \"loan_status_current_code\", \"loan_purpose_category\"]\n",
    "embed_columns = [\"borrower_address_zip\"] + one_hot_encoding_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429664aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(103810, 89)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_pipe = SkPipeline([\n",
    "    ('extract', NumericExtractor()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "date_pipe = SkPipeline([\n",
    "    ('cyclical', CyclicalDateEncoder()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()) # scaling is needed for the year, not for sin/cos\n",
    "])\n",
    "\n",
    "binary_pipe = SkPipeline([\n",
    "    ('binary_enc', BinaryModeEncoder()), \n",
    "    ('impute', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "# remainder columns are numerical\n",
    "remainder_pipe = SkPipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_pipe', numeric_pipe, extract_fields),\n",
    "    ('date_pipe', date_pipe, date_fields),\n",
    "    ('bin_pipe', binary_pipe, binary_cols),\n",
    "    ('drop_redundant', 'drop', redundant_cols),\n",
    "    ],\n",
    "    remainder=remainder_pipe,\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "numerical_pipeline = SkPipeline([\n",
    "    ('dropper', HighMissingDropper(threshold=20)),\n",
    "    ('prep', preprocessor),\n",
    "])\n",
    "\n",
    "numerical_pipeline.set_output(transform=\"pandas\")\n",
    "\n",
    "numerical_columns = [c for c in X_train.columns if c not in embed_columns]\n",
    "\n",
    "X_numerical_train = numerical_pipeline.fit_transform(X_train[numerical_columns], y_train)\n",
    "X_numerical_val = numerical_pipeline.transform(X_val[numerical_columns]) \n",
    "X_numerical_test = numerical_pipeline.transform(X_test[numerical_columns])\n",
    "\n",
    "print(type(X_numerical_train))\n",
    "print(X_numerical_train.shape)\n",
    "\n",
    "# join all in train_df\n",
    "train_df = pd.concat([X_numerical_train, X_train[embed_columns]], axis=1)\n",
    "train_df['grade'] = y_train\n",
    "\n",
    "val_df = pd.concat([X_numerical_val, X_val[embed_columns]], axis=1)\n",
    "val_df['grade'] = y_val\n",
    "\n",
    "test_df = pd.concat([X_numerical_test, X_test[embed_columns]], axis=1)\n",
    "test_df['grade'] = y_test\n",
    "\n",
    "# class weights\n",
    "classes = np.unique(train_df[\"grade\"])\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df[\"grade\"])\n",
    "weight_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "weighted_loss = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eb8aa2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = embed_columns\n",
    "\n",
    "# Continuous columns are everything else in the processed df (excluding target and categoricals)\n",
    "target_col = \"grade\"\n",
    "continuous_cols = [\n",
    "    c for c in train_df.columns \n",
    "    if c not in categorical_cols and c != target_col\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3eb51d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: The \"Small & Speedy\" Model\n",
    "space_small = {\n",
    "    \"model_config__n_d\": 8,\n",
    "    \"model_config__n_a\": 8,\n",
    "    \"model_config__n_steps\": 3,\n",
    "    \"model_config__gamma\": 1.2,\n",
    "}\n",
    "\n",
    "# Scenario 2: The \"Medium & Balanced\" Model\n",
    "space_medium = {\n",
    "    \"model_config__n_d\": 16,\n",
    "    \"model_config__n_a\": 16,\n",
    "    \"model_config__n_steps\": 5,\n",
    "    \"model_config__gamma\": 1.3,\n",
    "}\n",
    "\n",
    "# Scenario 3: The \"Large & Deep\" Model\n",
    "space_large = {\n",
    "    \"model_config__n_d\": 32,\n",
    "    \"model_config__n_a\": 32,\n",
    "    \"model_config__n_steps\": 7,\n",
    "    \"model_config__gamma\": 1.5,\n",
    "}\n",
    "\n",
    "search_space = [space_small, space_medium, space_large]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d9254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 00:29:27,192 - {pytorch_tabular.tabular_model:145} - INFO - Experiment Tracking is turned off\n",
      "Seed set to 42\n",
      "2026-02-02 00:29:27,204 - {pytorch_tabular.tabular_model:547} - INFO - Preparing the DataLoaders\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model config 1/3:\n",
      "{'model_config__n_d': 8, 'model_config__n_a': 8, 'model_config__n_steps': 3, 'model_config__gamma': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 00:29:27,308 - {pytorch_tabular.tabular_datamodule:527} - INFO - Setting up the datamodule for classification task\n",
      "2026-02-02 00:29:27,478 - {pytorch_tabular.tabular_model:598} - INFO - Preparing the Model: TabNetModel\n",
      "2026-02-02 00:29:27,675 - {pytorch_tabular.tabular_model:341} - INFO - Preparing the Trainer\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2026-02-02 00:29:27,691 - {pytorch_tabular.tabular_model:677} - INFO - Training Started\n",
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:881: Checkpoint directory /Users/geko/unibo/data_analytics/project/saved_models exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ custom_loss      │ CrossEntropyLoss │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Identity         │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _backbone        │ TabNetBackbone   │ 58.6 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ _head            │ Identity         │      0 │ train │     0 │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ custom_loss      │ CrossEntropyLoss │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │ 58.6 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │     0 │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 58.6 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 58.6 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 113                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 58.6 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 58.6 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 113                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f327e9d1d45a4cdfbfa94ca6a61e16af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing\n",
       "the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing\n",
       "the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: \n",
       "UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
       "  warnings.warn(warn_msg)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: \n",
       "UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
       "  warnings.warn(warn_msg)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 00:29:45,959 - {pytorch_tabular.tabular_model:690} - INFO - Training the model completed\n",
      "2026-02-02 00:29:45,959 - {pytorch_tabular.tabular_model:1531} - INFO - Loading the best model\n",
      "2026-02-02 00:29:46,942 - {pytorch_tabular.tabular_model:145} - INFO - Experiment Tracking is turned off\n",
      "Seed set to 42\n",
      "2026-02-02 00:29:46,953 - {pytorch_tabular.tabular_model:547} - INFO - Preparing the DataLoaders\n",
      "2026-02-02 00:29:47,020 - {pytorch_tabular.tabular_datamodule:527} - INFO - Setting up the datamodule for classification task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 0 Result: {'config_id': 0, 'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.2, 'test_acc': 0.17015609723205555, 'test_bacc': 0.17575241386113113, 'test_f1': 0.17149887123141289}\n",
      "-> Saved progress to tabnet_grid_search_running.csv\n",
      "----------------------------------------\n",
      "Training model config 2/3:\n",
      "{'model_config__n_d': 16, 'model_config__n_a': 16, 'model_config__n_steps': 5, 'model_config__gamma': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 00:29:47,195 - {pytorch_tabular.tabular_model:598} - INFO - Preparing the Model: TabNetModel\n",
      "2026-02-02 00:29:47,400 - {pytorch_tabular.tabular_model:341} - INFO - Preparing the Trainer\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2026-02-02 00:29:47,415 - {pytorch_tabular.tabular_model:677} - INFO - Training Started\n",
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:881: Checkpoint directory /Users/geko/unibo/data_analytics/project/saved_models exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ custom_loss      │ CrossEntropyLoss │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Identity         │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _backbone        │ TabNetBackbone   │ 93.4 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ _head            │ Identity         │      0 │ train │     0 │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ custom_loss      │ CrossEntropyLoss │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │ 93.4 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │     0 │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 93.4 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 93.4 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 161                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 93.4 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 93.4 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 161                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be65f44b9934a479e351feda908d461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing\n",
       "the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing\n",
       "the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: \n",
       "UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
       "  warnings.warn(warn_msg)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: \n",
       "UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
       "  warnings.warn(warn_msg)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 00:30:12,173 - {pytorch_tabular.tabular_model:690} - INFO - Training the model completed\n",
      "2026-02-02 00:30:12,173 - {pytorch_tabular.tabular_model:1531} - INFO - Loading the best model\n",
      "2026-02-02 00:30:13,954 - {pytorch_tabular.tabular_model:145} - INFO - Experiment Tracking is turned off\n",
      "Seed set to 42\n",
      "2026-02-02 00:30:13,964 - {pytorch_tabular.tabular_model:547} - INFO - Preparing the DataLoaders\n",
      "2026-02-02 00:30:14,045 - {pytorch_tabular.tabular_datamodule:527} - INFO - Setting up the datamodule for classification task\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 1 Result: {'config_id': 1, 'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.3, 'test_acc': 0.15828866187923535, 'test_bacc': 0.156258368963933, 'test_f1': 0.16936865283433108}\n",
      "-> Saved progress to tabnet_grid_search_running.csv\n",
      "----------------------------------------\n",
      "Training model config 3/3:\n",
      "{'model_config__n_d': 32, 'model_config__n_a': 32, 'model_config__n_steps': 7, 'model_config__gamma': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 00:30:14,267 - {pytorch_tabular.tabular_model:598} - INFO - Preparing the Model: TabNetModel\n",
      "2026-02-02 00:30:14,522 - {pytorch_tabular.tabular_model:341} - INFO - Preparing the Trainer\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2026-02-02 00:30:14,538 - {pytorch_tabular.tabular_model:677} - INFO - Training Started\n",
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:881: Checkpoint directory /Users/geko/unibo/data_analytics/project/saved_models exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ custom_loss      │ CrossEntropyLoss │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Identity         │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _backbone        │ TabNetBackbone   │  235 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ _head            │ Identity         │      0 │ train │     0 │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ custom_loss      │ CrossEntropyLoss │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Identity         │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabNetBackbone   │  235 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ _head            │ Identity         │      0 │ train │     0 │\n",
       "└───┴──────────────────┴──────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 235 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 235 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 209                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 235 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 235 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 209                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c273270cf74009bd9c3839e60d1655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing\n",
       "the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing\n",
       "the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: \n",
       "UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
       "  warnings.warn(warn_msg)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: \n",
       "UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
       "  warnings.warn(warn_msg)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 00:30:44,940 - {pytorch_tabular.tabular_model:690} - INFO - Training the model completed\n",
      "2026-02-02 00:30:44,940 - {pytorch_tabular.tabular_model:1531} - INFO - Loading the best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 2 Result: {'config_id': 2, 'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.5, 'test_acc': 0.139813222750413, 'test_bacc': 0.15536913096043328, 'test_f1': 0.15313400298804045}\n",
      "-> Saved progress to tabnet_grid_search_running.csv\n",
      "----------------------------------------\n",
      "Final Comparison:\n",
      "   config_id  n_d  n_a  n_steps  gamma  test_acc  test_bacc   test_f1\n",
      "0          0    8    8        3    1.2  0.170156   0.175752  0.171499\n",
      "1          1   16   16        5    1.3  0.158289   0.156258  0.169369\n",
      "2          2   32   32        7    1.5  0.139813   0.155369  0.153134\n",
      "Results saved to tabnet_grid_search_final.csv\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "results_log = []\n",
    "backup_file = \"tabnet_grid_search_running.csv\"\n",
    "\n",
    "for i, space in enumerate(search_space):\n",
    "    print(f\"Training model config {i+1}/{len(search_space)}:\")\n",
    "    print(space)\n",
    "    \n",
    "    n_d = space[\"model_config__n_d\"]\n",
    "    n_a = space[\"model_config__n_a\"]\n",
    "    n_steps = space[\"model_config__n_steps\"]\n",
    "    gamma = space[\"model_config__gamma\"]\n",
    "\n",
    "    data_config = DataConfig(\n",
    "        target=[\"grade\"], \n",
    "        continuous_cols=continuous_cols,\n",
    "        categorical_cols=categorical_cols,\n",
    "        normalize_continuous_features=False,\n",
    "    )\n",
    "\n",
    "    trainer_config = TrainerConfig(\n",
    "        batch_size=512,\n",
    "        max_epochs=EPOCHS,\n",
    "        early_stopping=\"valid_loss\",\n",
    "        early_stopping_patience=15,\n",
    "        accelerator=\"auto\",\n",
    "    )\n",
    "\n",
    "    model_config = TabNetModelConfig(\n",
    "        task=\"classification\",\n",
    "        metrics=['accuracy'], \n",
    "        n_d=n_d,\n",
    "        n_a=n_a,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma\n",
    "    )\n",
    "\n",
    "    optimizer_config = OptimizerConfig(\n",
    "        optimizer=\"AdamW\",\n",
    "        lr_scheduler=\"ReduceLROnPlateau\",\n",
    "        lr_scheduler_params={\"mode\": \"min\", \"factor\": 0.1, \"patience\": 10, \"min_lr\": 1e-5}\n",
    "    )\n",
    "\n",
    "    tabular_model = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "    )\n",
    "    \n",
    "    tabular_model.fit(train=train_df, validation=val_df, loss=weighted_loss)\n",
    "\n",
    "    pred_df = tabular_model.predict(test_df)\n",
    "    \n",
    "    y_true = test_df[\"grade\"]\n",
    "    \n",
    "    y_pred = pred_df[\"grade_prediction\"] \n",
    "    \n",
    "    manual_acc = accuracy_score(y_true, y_pred)\n",
    "    manual_bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    manual_f1 = f1_score(y_true, y_pred, average=\"weighted\") \n",
    "\n",
    "    run_metrics = {\n",
    "        \"config_id\": i,\n",
    "        \"n_d\": n_d,\n",
    "        \"n_a\": n_a,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"test_acc\": manual_acc,\n",
    "        \"test_bacc\": manual_bacc,\n",
    "        \"test_f1\": manual_f1,\n",
    "    }\n",
    "    results_log.append(run_metrics)\n",
    "    print(f\"Config {i} Result: {run_metrics}\")\n",
    "    \n",
    "    pd.DataFrame(results_log).to_csv(backup_file, index=False)\n",
    "    print(f\"-> Saved progress to {backup_file}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "summary_df = pd.DataFrame(results_log)\n",
    "final_filename = \"tabnet_grid_search_final.csv\"\n",
    "summary_df.to_csv(final_filename, index=False)\n",
    "\n",
    "print(\"Final Comparison:\")\n",
    "print(summary_df.sort_values(by=\"test_bacc\", ascending=False))\n",
    "print(f\"Results saved to {final_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
