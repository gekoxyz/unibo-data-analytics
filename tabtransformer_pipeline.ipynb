{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb68583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from pytorch_tabular.models import TabTransformerConfig\n",
    "from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig, ExperimentConfig\n",
    "from pytorch_tabular import TabularModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e0e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device is available.\n",
      "Shape of the dataset: (148301, 145)\n",
      "Number of duplicates in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS device is available.\")\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"CUDA device is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No GPU acceleration available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Fix the seed to have deterministic behaviour\n",
    "def fix_random(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "SEED = 1337\n",
    "fix_random(SEED)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "DATASET_PATH = \"dataset_train/dataset.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH, delimiter=\",\")\n",
    "\n",
    "print(f\"Shape of the dataset: {dataset.shape}\")\n",
    "duplicates = dataset[dataset.duplicated()]\n",
    "print(f\"Number of duplicates in the dataset: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6d6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=[\"grade\"])\n",
    "y = dataset[\"grade\"].map({\"A\": 6, \"B\": 5, \"C\": 4, \"D\": 3, \"E\": 2, \"F\": 1, \"G\": 0})\n",
    "\n",
    "# 1. First Split: Separate out the final Hold-out Test set (e.g., 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Second Split: Separate Train from Validation (e.g., 10% of total, or 12.5% of the temp data)\n",
    "# This ensures Validation data is never seen by the \"fit\" method\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.125, stratify=y_temp\n",
    ")\n",
    "# Resulting Ratios roughly: Train (70%), Val (10%), Test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8daf6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts integers from strings using regex\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "        return X\n",
    "    \n",
    "    def set_output(self, *, transform=None):\n",
    "        # We ignore the 'transform' argument because this class \n",
    "        # always returns a DataFrame (pandas) by design.\n",
    "        return self\n",
    "\n",
    "class CyclicalDateEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Converts mm-yyyy to year + sine/cosine month encoding.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            # errors=\"coerce\" turns unparseable data/NaNs into NaT\n",
    "            date_series = pd.to_datetime(X[col], format=\"%b-%Y\", errors=\"coerce\")\n",
    "            # If date is NaT, these become NaN, which we handle in the pipeline later\n",
    "            angle = 2 * np.pi * date_series.dt.month / 12\n",
    "\n",
    "            X[f\"{col}_year\"] = date_series.dt.year\n",
    "            X[f\"{col}_month_sin\"] = np.sin(angle)\n",
    "            X[f\"{col}_month_cos\"] = np.cos(angle)\n",
    "            \n",
    "            X.drop(columns=[col], inplace=True)\n",
    "        return X\n",
    "    \n",
    "    def set_output(self, *, transform=None):\n",
    "        # We ignore the 'transform' argument because this class \n",
    "        # always returns a DataFrame (pandas) by design.\n",
    "        return self\n",
    "    \n",
    "class BinaryModeEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\"Encodes 0 if value is mode, 1 if not\"\"\"\n",
    "    def __init__(self):\n",
    "        self.modes_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate mode for each column and store it\n",
    "        for col in X.columns:\n",
    "            self.modes_[col] = X[col].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col, mode in self.modes_.items():\n",
    "            # Apply: 1 if NOT the mode (least frequent), 0 if mode\n",
    "            X_copy[col] = (X_copy[col] != mode).astype(int)\n",
    "        return X_copy\n",
    "    \n",
    "    def set_output(self, *, transform=None):\n",
    "        # We ignore the 'transform' argument because this class \n",
    "        # always returns a DataFrame (pandas) by design.\n",
    "        return self\n",
    "    \n",
    "class HighMissingDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns with high missing percentage. Fits only on training data.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=20):\n",
    "        self.threshold = threshold\n",
    "        self.cols_to_drop_ = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        missing_percentages = X.isna().mean() * 100\n",
    "        self.cols_to_drop_ = missing_percentages[missing_percentages > self.threshold].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.cols_to_drop_)\n",
    "    \n",
    "    def set_output(self, *, transform=None):\n",
    "        # We ignore the 'transform' argument because this class \n",
    "        # always returns a DataFrame (pandas) by design.\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d581d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = [\"loan_title\", \"borrower_address_state\"]\n",
    "\n",
    "binary_cols = [\"loan_payment_plan_flag\", \"listing_initial_status\", \"application_type_label\",\n",
    "               \"hardship_flag_indicator\", \"disbursement_method_type\", \"debt_settlement_flag_indicator\"]\n",
    "extract_fields = [\"loan_contract_term_months\", \"borrower_profile_employment_length\"]\n",
    "date_fields = [\"loan_issue_date\", \"credit_history_earliest_line\", \"last_payment_date\", \"last_credit_pull_date\"]\n",
    "\n",
    "# instead of one hot + embedding we just embed\n",
    "one_hot_encoding_cols = [\"borrower_housing_ownership_status\", \"borrower_income_verification_status\",\n",
    "                       \"loan_status_current_code\", \"loan_purpose_category\"]\n",
    "embed_columns = [\"borrower_address_zip\"] + one_hot_encoding_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "429664aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(103810, 89)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_pipe = SkPipeline([\n",
    "    ('extract', NumericExtractor()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "date_pipe = SkPipeline([\n",
    "    ('cyclical', CyclicalDateEncoder()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()) # scaling is needed for the year, not for sin/cos\n",
    "])\n",
    "\n",
    "binary_pipe = SkPipeline([\n",
    "    ('binary_enc', BinaryModeEncoder()), \n",
    "    ('impute', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "# remainder columns are numerical\n",
    "remainder_pipe = SkPipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_pipe', numeric_pipe, extract_fields),\n",
    "    ('date_pipe', date_pipe, date_fields),\n",
    "    ('bin_pipe', binary_pipe, binary_cols),\n",
    "    ('drop_redundant', 'drop', redundant_cols),\n",
    "    ],\n",
    "    remainder=remainder_pipe,\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "numerical_pipeline = SkPipeline([\n",
    "    ('dropper', HighMissingDropper(threshold=20)),\n",
    "    ('prep', preprocessor),\n",
    "])\n",
    "\n",
    "numerical_pipeline.set_output(transform=\"pandas\")\n",
    "\n",
    "numerical_columns = [c for c in X_train.columns if c not in embed_columns]\n",
    "\n",
    "X_numerical_train = numerical_pipeline.fit_transform(X_train[numerical_columns], y_train)\n",
    "X_numerical_val = numerical_pipeline.transform(X_val[numerical_columns]) \n",
    "X_numerical_test = numerical_pipeline.transform(X_test[numerical_columns])\n",
    "\n",
    "print(type(X_numerical_train))\n",
    "print(X_numerical_train.shape)\n",
    "\n",
    "# join all in train_df\n",
    "train_df = pd.concat([X_numerical_train, X_train[embed_columns]], axis=1)\n",
    "train_df['grade'] = y_train\n",
    "\n",
    "val_df = pd.concat([X_numerical_val, X_val[embed_columns]], axis=1)\n",
    "val_df['grade'] = y_val\n",
    "\n",
    "test_df = pd.concat([X_numerical_test, X_test[embed_columns]], axis=1)\n",
    "test_df['grade'] = y_test\n",
    "\n",
    "# class weights\n",
    "classes = np.unique(train_df[\"grade\"])\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=train_df[\"grade\"])\n",
    "weight_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "weighted_loss = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8aa2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = embed_columns\n",
    "\n",
    "# Continuous columns are everything else in the processed df (excluding target and categoricals)\n",
    "target_col = \"grade\"\n",
    "continuous_cols = [\n",
    "    c for c in train_df.columns \n",
    "    if c not in categorical_cols and c != target_col\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a6d9254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "2026-02-02 23:52:24,233 - {pytorch_tabular.tabular_model:547} - INFO - Preparing the DataLoaders\n",
      "2026-02-02 23:52:24,288 - {pytorch_tabular.tabular_datamodule:527} - INFO - Setting up the datamodule for classification task\n",
      "2026-02-02 23:52:24,448 - {pytorch_tabular.tabular_model:598} - INFO - Preparing the Model: TabTransformerModel\n",
      "2026-02-02 23:52:24,557 - {pytorch_tabular.tabular_model:341} - INFO - Preparing the Trainer\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2026-02-02 23:52:24,566 - {pytorch_tabular.tabular_model:677} - INFO - Training Started\n",
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:881: Checkpoint directory /Users/geko/unibo/data_analytics/project/saved_models exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ custom_loss      │ CrossEntropyLoss       │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _backbone        │ TabTransformerBackbone │ 14.7 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ _embedding_layer │ Embedding2dLayer       │ 14.7 K │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ _head            │ LinearHead             │  1.2 K │ train │     0 │\n",
       "└───┴──────────────────┴────────────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ custom_loss      │ CrossEntropyLoss       │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ TabTransformerBackbone │ 14.7 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding2dLayer       │ 14.7 K │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ _head            │ LinearHead             │  1.2 K │ train │     0 │\n",
       "└───┴──────────────────┴────────────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 30.6 K                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 30.6 K                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 57                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 30.6 K                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 30.6 K                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 57                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb180464f04cb99f9738771cce3db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing\n",
       "the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing\n",
       "the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: \n",
       "UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
       "  warnings.warn(warn_msg)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: \n",
       "UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
       "  warnings.warn(warn_msg)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/da\n",
       "ta_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider \n",
       "increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 23:58:14,740 - {pytorch_tabular.tabular_model:690} - INFO - Training the model completed\n",
      "2026-02-02 23:58:14,741 - {pytorch_tabular.tabular_model:1531} - INFO - Loading the best model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'test_acc': 0.8197970398840227, 'test_bacc': 0.7612215767935024, 'test_f1': 0.8201634592477411}\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "hyperparameters = {\n",
    "   \"input_embed_dim\": 16,\n",
    "   \"num_heads\": 4,\n",
    "   \"num_attn_blocks\": 2,\n",
    "}\n",
    "\n",
    "results_log = []\n",
    "\n",
    "input_embed_dim = hyperparameters[\"input_embed_dim\"]\n",
    "num_heads = hyperparameters[\"num_heads\"]\n",
    "num_attn_blocks = hyperparameters[\"num_attn_blocks\"]\n",
    "\n",
    "data_config = DataConfig(\n",
    "    target=[\"grade\"], \n",
    "    continuous_cols=continuous_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    "    normalize_continuous_features=False,\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=512,\n",
    "    max_epochs=EPOCHS,\n",
    "    early_stopping=\"valid_loss\",\n",
    "    early_stopping_patience=15,\n",
    "    accelerator=\"auto\",\n",
    ")\n",
    "\n",
    "model_config = TabTransformerConfig(\n",
    "    task=\"classification\",\n",
    "    metrics=['accuracy'], \n",
    "    input_embed_dim=input_embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_attn_blocks=num_attn_blocks\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer=\"AdamW\",\n",
    "    lr_scheduler=\"ReduceLROnPlateau\",\n",
    "    lr_scheduler_params={\"mode\": \"min\", \"factor\": 0.1, \"patience\": 10, \"min_lr\": 1e-6}\n",
    ")\n",
    "\n",
    "experiment_config = ExperimentConfig(\n",
    "    project_name=\"DataAnalytics\",\n",
    "    run_name=\"TabTransformer\",\n",
    "    log_target=\"tensorboard\",\n",
    ")\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    "    experiment_config=experiment_config\n",
    ")\n",
    "\n",
    "tabular_model.fit(train=train_df, validation=val_df, loss=weighted_loss)\n",
    "\n",
    "pred_df = tabular_model.predict(test_df)\n",
    "\n",
    "y_true = test_df[\"grade\"]\n",
    "y_pred = pred_df[\"grade_prediction\"] \n",
    "\n",
    "manual_acc = accuracy_score(y_true, y_pred)\n",
    "manual_bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "manual_f1 = f1_score(y_true, y_pred, average=\"weighted\") \n",
    "\n",
    "run_metrics = {\n",
    "    \"test_acc\": manual_acc,\n",
    "    \"test_bacc\": manual_bacc,\n",
    "    \"test_f1\": manual_f1,\n",
    "}\n",
    "results_log.append(run_metrics)\n",
    "print(f\"Result: {run_metrics}\")\n",
    "\n",
    "summary_df = pd.DataFrame(results_log)\n",
    "final_filename = \"tabtransformer_performance.csv\"\n",
    "summary_df.to_csv(final_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1332065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`weights_only` was not set, defaulting to `False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to saved_tabtransformer_model\n"
     ]
    }
   ],
   "source": [
    "save_path = \"saved_tabtransformer_model\"\n",
    "tabular_model.save_model(save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
