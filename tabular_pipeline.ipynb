{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32640029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Replaces sklearn Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import TabNetModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88640189",
   "metadata": {},
   "source": [
    "# load the dataset and fix values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a6eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device is available.\n",
      "Shape of the dataset: (148301, 145)\n",
      "Number of duplicates in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS device is available.\")\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"CUDA device is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No GPU acceleration available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Fix the seed to have deterministic behaviour\n",
    "def fix_random(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "SEED = 1337\n",
    "fix_random(SEED)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "DATASET_PATH = \"dataset_train/dataset.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH, delimiter=\",\")\n",
    "\n",
    "print(f\"Shape of the dataset: {dataset.shape}\")\n",
    "duplicates = dataset[dataset.duplicated()]\n",
    "print(f\"Number of duplicates in the dataset: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae26d9",
   "metadata": {},
   "source": [
    "## split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe853ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=[\"grade\"])\n",
    "y = dataset[\"grade\"].map({\"A\": 6, \"B\": 5, \"C\": 4, \"D\": 3, \"E\": 2, \"F\": 1, \"G\": 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = dataset.select_dtypes(include=['object', 'category']).columns\n",
    "print(f\"Categorical columns:\\n{categorical_cols.sort_values()}\")\n",
    "numerical_cols = dataset.select_dtypes(include=['number']).columns\n",
    "print(f\"Numerical columns:\\n{numerical_cols.sort_values()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e362af",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = ['loan_title', \"borrower_address_state\"]\n",
    "binary_cols = [\"loan_payment_plan_flag\", \"listing_initial_status\", \"application_type_label\",\n",
    "               \"hardship_flag_indicator\", \"disbursement_method_type\", \"debt_settlement_flag_indicator\"]\n",
    "one_hot_encoding_cols = [\"borrower_housing_ownership_status\", \"borrower_income_verification_status\",\n",
    "                       \"loan_status_current_code\", \"loan_purpose_category\"]\n",
    "extract_fields = [\"loan_contract_term_months\", \"borrower_profile_employment_length\"]\n",
    "date_fields = [\"loan_issue_date\", \"credit_history_earliest_line\", \"last_payment_date\", \"last_credit_pull_date\"]\n",
    "embed_column = ['borrower_address_zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")\n",
    "\n",
    "data_config = DataConfig(\n",
    "    target=[\n",
    "        \"target\"\n",
    "    ],  # target should always be a list.\n",
    "    continuous_cols=num_col_names,\n",
    "    categorical_cols=cat_col_names,\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,  # Runs the LRFinder to automatically derive a learning rate\n",
    "    batch_size=1024,\n",
    "    max_epochs=100,\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=\"1024-512-512\",  # Number of nodes in each layer\n",
    "    activation=\"LeakyReLU\",  # Activation between each layers\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "tabular_model.fit(train=train, validation=val)\n",
    "result = tabular_model.evaluate(test)\n",
    "pred_df = tabular_model.predict(test)\n",
    "tabular_model.save_model(\"examples/basic\")\n",
    "loaded_model = TabularModel.load_model(\"examples/basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7472a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Configuration\n",
    "data_config = DataConfig(\n",
    "    target=[\"grade\"],\n",
    "    continuous_cols=[\"age\", \"income\", \"credit_score\"],\n",
    "    categorical_cols=[\"gender\", \"zip_code\", \"occupation\"],\n",
    "    normalize_continuous_features=True, # Critical for TabNet\n",
    "    continuous_feature_transform=\"quantile_normal\", # Gaussian Rank for stability\n",
    "    handle_unknown_categories=True, # Robustness for inference\n",
    "    num_workers=4, # Parallel loading\n",
    "    pin_memory=True # GPU optimization\n",
    ")\n",
    "\n",
    "# 2. Trainer Configuration\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024, # Large batch size for Ghost BN\n",
    "    max_epochs=100, # Allow long training with early stopping\n",
    "    accelerator=\"auto\", # Auto-detect GPU\n",
    "    early_stopping_patience=10, # Stop after 10 epochs of no improvement\n",
    "    checkpoints_save_top_k=1, # Save best model\n",
    "    load_best=True, # Auto-load best weights\n",
    "    auto_lr_find=True # Enable LR Finder (optional)\n",
    ")\n",
    "\n",
    "# 3. Optimizer Configuration\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer=\"AdamW\", # Decoupled weight decay\n",
    "    optimizer_params={\"weight_decay\": 0.01},\n",
    "    lr_scheduler=\"ReduceLROnPlateau\",\n",
    "    lr_scheduler_params={\"patience\": 3, \"factor\": 0.1},\n",
    "    lr_scheduler_monitor_metric=\"valid_loss\"\n",
    ")\n",
    "\n",
    "# 4. TabNet Model Configuration\n",
    "model_config = TabNetModelConfig(\n",
    "    task=\"classification\",\n",
    "    learning_rate=0.02, # Initial estimate\n",
    "    n_d=16, # Decision dimension\n",
    "    n_a=16, # Attention dimension\n",
    "    n_steps=5, # 5 sequential decision steps\n",
    "    gamma=1.2, # Relaxation parameter for sparsity\n",
    "    n_independent=2, # Independent GLU blocks\n",
    "    n_shared=2, # Shared GLU blocks\n",
    "    virtual_batch_size=128, # Ghost BN size\n",
    "    metrics=[\"accuracy\", \"f1_score\"], # Custom metrics\n",
    "    metrics_prob_input=[False, False] # Both metrics expect class labels, not probs\n",
    ")\n",
    "\n",
    "# 5. Experiment Configuration (WandB)\n",
    "experiment_config = ExperimentConfig(\n",
    "    project_name=\"tabnet-churn-prediction\",\n",
    "    run_name=\"tabnet_experiment_01\",\n",
    "    exp_watch=\"gradients\", # Track gradients for debugging\n",
    "    log_logits=True # Track output distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Orchestrator\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    "    experiment_config=experiment_config,\n",
    ")\n",
    "\n",
    "# Split Data (or use pre-split dataframes)\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Train\n",
    "# Note: WandB run starts automatically here\n",
    "tabular_model.fit(train=train_df)\n",
    "\n",
    "# Evaluate\n",
    "result = tabular_model.evaluate(test_df)\n",
    "print(f\"Test Metrics: {result}\")\n",
    "\n",
    "# Predict\n",
    "pred_df = tabular_model.predict(test_df)\n",
    "print(pred_df.head())\n",
    "\n",
    "# Save for Production\n",
    "tabular_model.save_model(\"production_tabnet\", inference_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a61d55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts integers from strings using regex\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "        return X\n",
    "\n",
    "class CyclicalDateEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Converts mm-yyyy to year + sine/cosine month encoding.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            # errors=\"coerce\" turns unparseable data/NaNs into NaT\n",
    "            date_series = pd.to_datetime(X[col], format=\"%b-%Y\", errors=\"coerce\")\n",
    "            # If date is NaT, these become NaN, which we handle in the pipeline later\n",
    "            angle = 2 * np.pi * date_series.dt.month / 12\n",
    "\n",
    "            X[f\"{col}_year\"] = date_series.dt.year\n",
    "            X[f\"{col}_month_sin\"] = np.sin(angle)\n",
    "            X[f\"{col}_month_cos\"] = np.cos(angle)\n",
    "            \n",
    "            X.drop(columns=[col], inplace=True)\n",
    "        return X\n",
    "    \n",
    "class BinaryModeEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\"Encodes 0 if value is mode, 1 if not\"\"\"\n",
    "    def __init__(self):\n",
    "        self.modes_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate mode for each column and store it\n",
    "        for col in X.columns:\n",
    "            self.modes_[col] = X[col].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col, mode in self.modes_.items():\n",
    "            # Apply: 1 if NOT the mode (least frequent), 0 if mode\n",
    "            X_copy[col] = (X_copy[col] != mode).astype(int)\n",
    "        return X_copy\n",
    "    \n",
    "class HighMissingDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns with high missing percentage. Fits only on training data.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=20):\n",
    "        self.threshold = threshold\n",
    "        self.cols_to_drop_ = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate missing percentages only on training data\n",
    "        missing_percentages = X.isna().mean() * 100\n",
    "        self.cols_to_drop_ = missing_percentages[missing_percentages > self.threshold].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.cols_to_drop_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821899ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = ['loan_title', \"borrower_address_state\"]\n",
    "binary_cols = [\"loan_payment_plan_flag\", \"listing_initial_status\", \"application_type_label\",\n",
    "               \"hardship_flag_indicator\", \"disbursement_method_type\", \"debt_settlement_flag_indicator\"]\n",
    "one_hot_encoding_cols = [\"borrower_housing_ownership_status\", \"borrower_income_verification_status\",\n",
    "                       \"loan_status_current_code\", \"loan_purpose_category\"]\n",
    "extract_fields = [\"loan_contract_term_months\", \"borrower_profile_employment_length\"]\n",
    "date_fields = [\"loan_issue_date\", \"credit_history_earliest_line\", \"last_payment_date\", \"last_credit_pull_date\"]\n",
    "embed_column = ['borrower_address_zip'] + one_hot_encoding_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c54be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118640, 121)\n",
      "(118640,)\n",
      "(118640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "numeric_pipe = SkPipeline([\n",
    "    ('extract', NumericExtractor()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = SkPipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "date_pipe = SkPipeline([\n",
    "    ('cyclical', CyclicalDateEncoder()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()) # scaling is needed for the year, not for sin/cos\n",
    "])\n",
    "\n",
    "binary_pipe = SkPipeline([\n",
    "    ('binary_enc', BinaryModeEncoder()), \n",
    "    ('impute', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_pipe', numeric_pipe, extract_fields),\n",
    "    ('date_pipe', date_pipe, date_fields),\n",
    "    ('bin_pipe', binary_pipe, binary_cols),\n",
    "    ('drop_redundant', 'drop', redundant_cols),\n",
    "], remainder=SimpleImputer(strategy='median'))\n",
    "\n",
    "numerical_pipeline = SkPipeline([\n",
    "    ('dropper', HighMissingDropper(threshold=20)),\n",
    "    ('prep', preprocessor),\n",
    "])\n",
    "\n",
    "embed_pipeline = SkPipeline([\n",
    "    ('dropper', HighMissingDropper(threshold=20)),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ordinal', OrdinalEncoder(dtype=np.int64, handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "numerical_columns = [c for c in X_train.columns if c not in embed_column]\n",
    "\n",
    "X_numerical_train = numerical_pipeline.fit_transform(X_train[numerical_columns], y_train)\n",
    "X_zip_train = embed_pipeline.fit_transform(X_train[embed_column]).squeeze()\n",
    "\n",
    "X_numerical_test = numerical_pipeline.transform(X_test[numerical_columns])\n",
    "X_zip_test = embed_pipeline.transform(X_test[embed_column]).squeeze()\n",
    "\n",
    "print(X_numerical_train.shape)\n",
    "print(X_zip_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14054da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Class Weights: tensor([3.4823, 2.8158, 1.7534, 0.9966, 0.5713, 0.5614, 0.7984],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# 1. Create tensors separately\n",
    "# Numerical features -> Float32\n",
    "X_num_train_tensor = torch.tensor(X_numerical_train, dtype=torch.float32)\n",
    "X_num_test_tensor = torch.tensor(X_numerical_test, dtype=torch.float32)\n",
    "\n",
    "# Zip codes (indices) -> Long (Integers) required for nn.Embedding\n",
    "X_zip_train_tensor = torch.tensor(X_zip_train, dtype=torch.long)\n",
    "X_zip_test_tensor = torch.tensor(X_zip_test, dtype=torch.long)\n",
    "\n",
    "# Targets -> Long (standard for Classification)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# 2. Pass ALL tensors to TensorDataset\n",
    "# The dataset will now yield a tuple of 3 items: (numerical_data, zip_data, label)\n",
    "train_dataset = TensorDataset(X_num_train_tensor, X_zip_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_num_test_tensor, X_zip_test_tensor, y_test_tensor)\n",
    "\n",
    "# 3. Create DataLoaders\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 4. Handle Class Weights (unchanged)\n",
    "y_train_np = y_train_tensor.numpy()\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_np),\n",
    "    y=y_train_np\n",
    ")\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"Computed Class Weights: {class_weights_tensor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
