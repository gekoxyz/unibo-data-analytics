{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32640029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline # Replaces sklearn Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88640189",
   "metadata": {},
   "source": [
    "# load the dataset and fix values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9a6eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device is available.\n",
      "Shape of the dataset: (148301, 145)\n",
      "Number of duplicates in the dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS device is available.\")\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    print(\"CUDA device is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No GPU acceleration available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Fix the seed to have deterministic behaviour\n",
    "def fix_random(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  # slower\n",
    "\n",
    "SEED = 1337\n",
    "fix_random(SEED)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "DATASET_PATH = \"dataset_train/dataset.csv\"\n",
    "dataset = pd.read_csv(DATASET_PATH, delimiter=\",\")\n",
    "\n",
    "print(f\"Shape of the dataset: {dataset.shape}\")\n",
    "duplicates = dataset[dataset.duplicated()]\n",
    "print(f\"Number of duplicates in the dataset: {duplicates.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae26d9",
   "metadata": {},
   "source": [
    "## split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbe853ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=[\"grade\"])\n",
    "y = dataset[\"grade\"].map({\"A\": 6, \"B\": 5, \"C\": 4, \"D\": 3, \"E\": 2, \"F\": 1, \"G\": 0})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a61d55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts integers from strings using regex\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            X[col] = X[col].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "        return X\n",
    "\n",
    "class CyclicalDateEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Converts mm-yyyy to year + sine/cosine month encoding.\"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            # errors=\"coerce\" turns unparseable data/NaNs into NaT\n",
    "            date_series = pd.to_datetime(X[col], format=\"%b-%Y\", errors=\"coerce\")\n",
    "            # If date is NaT, these become NaN, which we handle in the pipeline later\n",
    "            angle = 2 * np.pi * date_series.dt.month / 12\n",
    "\n",
    "            X[f\"{col}_year\"] = date_series.dt.year\n",
    "            X[f\"{col}_month_sin\"] = np.sin(angle)\n",
    "            X[f\"{col}_month_cos\"] = np.cos(angle)\n",
    "            \n",
    "            X.drop(columns=[col], inplace=True)\n",
    "        return X\n",
    "    \n",
    "class BinaryModeEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\"Encodes 0 if value is mode, 1 if not\"\"\"\n",
    "    def __init__(self):\n",
    "        self.modes_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Calculate mode for each column and store it\n",
    "        for col in X.columns:\n",
    "            self.modes_[col] = X[col].mode()[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col, mode in self.modes_.items():\n",
    "            # Apply: 1 if NOT the mode (least frequent), 0 if mode\n",
    "            X_copy[col] = (X_copy[col] != mode).astype(int)\n",
    "        return X_copy\n",
    "    \n",
    "class HighMissingDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns with high missing percentage. Fits only on training data.\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=20):\n",
    "        self.threshold = threshold\n",
    "        self.cols_to_drop_ = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        missing_percentages = X.isna().mean() * 100\n",
    "        self.cols_to_drop_ = missing_percentages[missing_percentages > self.threshold].index.tolist()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        return X.drop(columns=self.cols_to_drop_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "821899ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_cols = [\"loan_title\", \"borrower_address_state\"]\n",
    "binary_cols = [\"loan_payment_plan_flag\", \"listing_initial_status\", \"application_type_label\",\n",
    "               \"hardship_flag_indicator\", \"disbursement_method_type\", \"debt_settlement_flag_indicator\"]\n",
    "one_hot_encoding_cols = [\"borrower_housing_ownership_status\", \"borrower_income_verification_status\",\n",
    "                       \"loan_status_current_code\", \"loan_purpose_category\"]\n",
    "extract_fields = [\"loan_contract_term_months\", \"borrower_profile_employment_length\"]\n",
    "date_fields = [\"loan_issue_date\", \"credit_history_earliest_line\", \"last_payment_date\", \"last_credit_pull_date\"]\n",
    "embed_column = [\"borrower_address_zip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "597c54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "numeric_pipe = SkPipeline([\n",
    "    ('extract', NumericExtractor()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = SkPipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "date_pipe = SkPipeline([\n",
    "    ('cyclical', CyclicalDateEncoder()),\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()) # scaling is needed for the year, not for sin/cos\n",
    "])\n",
    "\n",
    "binary_pipe = SkPipeline([\n",
    "    ('binary_enc', BinaryModeEncoder()), \n",
    "    ('impute', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "# remainder columns are numerical\n",
    "remainder_pipe = SkPipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_pipe', numeric_pipe, extract_fields),\n",
    "    ('cat_pipe', categorical_pipe, one_hot_encoding_cols),\n",
    "    ('date_pipe', date_pipe, date_fields),\n",
    "    ('bin_pipe', binary_pipe, binary_cols),\n",
    "    ('drop_redundant', 'drop', redundant_cols),\n",
    "    ],\n",
    "    remainder=remainder_pipe\n",
    ")\n",
    "\n",
    "numerical_pipeline = SkPipeline([\n",
    "    ('dropper', HighMissingDropper(threshold=20)),\n",
    "    ('prep', preprocessor),\n",
    "])\n",
    "\n",
    "zip_pipeline = SkPipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ordinal', OrdinalEncoder(dtype=np.int64, handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "\n",
    "# X_numerical_train = numerical_pipeline.fit_transform(X_train[numerical_columns], y_train)\n",
    "# X_zip_train = zip_pipeline.fit_transform(X_train[embed_column]).squeeze()\n",
    "\n",
    "# X_numerical_test = numerical_pipeline.transform(X_test[numerical_columns])\n",
    "# X_zip_test = zip_pipeline.transform(X_test[embed_column]).squeeze()\n",
    "\n",
    "# print(X_numerical_train.shape)\n",
    "# print(X_zip_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4673776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X_num, X_zip, y):\n",
    "    # Numerical features: Float32\n",
    "    x_num_t = torch.tensor(X_num, dtype=torch.float32)\n",
    "    \n",
    "    # Zip indices: Long (Int64). \n",
    "    # Shift by +1 so -1 (unknown) becomes 0.\n",
    "    x_zip_t = torch.tensor(X_zip, dtype=torch.long) + 1\n",
    "    \n",
    "    # Targets: Long for loss calculation\n",
    "    y_t = torch.tensor(y.values, dtype=torch.long)\n",
    "    \n",
    "    return TensorDataset(x_num_t, x_zip_t, y_t)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def run_cv_grid_search(model_class, param_grid, X_raw, y_raw, n_splits=3):\n",
    "    stratified_k_fold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    parameter_grid = list(ParameterGrid(param_grid))\n",
    "    results = [] \n",
    "    print(f\"Starting Grid Search | {len(parameter_grid)} Combos | {n_splits}-Fold CV\")\n",
    "\n",
    "    for i, params in enumerate(parameter_grid):\n",
    "        print(f\"Fitting Combo {i+1}/{len(parameter_grid)}: {params}\")\n",
    "        \n",
    "        # Store the metrics for the BEST epoch in each fold\n",
    "        fold_baccs = [] \n",
    "        fold_accs = []\n",
    "        fold_f1s = []\n",
    "        fold_times = [] # Optional: Add timing here if you want\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(stratified_k_fold.split(X_raw, y_raw)):\n",
    "\n",
    "            numerical_columns = [c for c in X_raw.columns if c not in embed_column]\n",
    "            \n",
    "            X_train_fold_raw = X_raw.iloc[train_idx]\n",
    "            y_train_fold = y_raw.iloc[train_idx]\n",
    "            \n",
    "            X_val_fold_raw = X_raw.iloc[val_idx]\n",
    "            y_val_fold = y_raw.iloc[val_idx]\n",
    "\n",
    "            fold_num_pipe = clone(numerical_pipeline)\n",
    "            fold_zip_pipe = clone(zip_pipeline)\n",
    "\n",
    "            X_num_train = fold_num_pipe.fit_transform(X_train_fold_raw[numerical_columns], y_train_fold)\n",
    "            X_zip_train = fold_zip_pipe.fit_transform(X_train_fold_raw[embed_column]).squeeze()\n",
    "\n",
    "            X_num_val = fold_num_pipe.transform(X_val_fold_raw[numerical_columns])\n",
    "            X_zip_val = fold_zip_pipe.transform(X_val_fold_raw[embed_column]).squeeze()\n",
    "\n",
    "            \n",
    "            fold_weights = compute_class_weight('balanced', classes=np.unique(y_train_fold), y=y_train_fold)\n",
    "            fold_weights_tensor = torch.tensor(fold_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "            train_ds = create_dataset(X_num_train, X_zip_train, y_train_fold)\n",
    "            val_ds = create_dataset(X_num_val, X_zip_val, y_val_fold)\n",
    "            train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            fold_params = params.copy()\n",
    "            fold_params['cont_dim'] = X_num_train.shape[1]\n",
    "    \n",
    "            # --- Model Setup ---\n",
    "            model = model_class(**fold_params).to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = nn.CrossEntropyLoss(weight=fold_weights_tensor)\n",
    "            \n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, mode='min', factor=0.1, patience=10, min_lr=1e-6\n",
    "            )\n",
    "\n",
    "            # --- TRACKING BEST SCORE FOR THIS FOLD ---\n",
    "            best_fold_bacc = -float('inf') \n",
    "            best_fold_acc = 0.0\n",
    "            best_fold_f1 = 0.0\n",
    "\n",
    "            start_time = time.time()\n",
    "            for epoch in range(EPOCHS):\n",
    "                model.train()\n",
    "                for b_cont, b_zip, b_y in train_loader:\n",
    "                    b_cont, b_zip, b_y = b_cont.to(device), b_zip.to(device), b_y.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = criterion(model(b_cont, b_zip), b_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                all_preds = []\n",
    "                all_targets = []\n",
    "                with torch.no_grad():\n",
    "                    for b_cont, b_zip, b_y in val_loader:\n",
    "                        b_cont, b_zip, b_y = b_cont.to(device), b_zip.to(device), b_y.to(device)\n",
    "                        logits = model(b_cont, b_zip)\n",
    "                        val_loss += criterion(logits, b_y).item()\n",
    "                        \n",
    "                        preds = torch.argmax(logits, dim=1)\n",
    "                        all_preds.extend(preds.cpu().numpy())\n",
    "                        all_targets.extend(b_y.cpu().numpy())\n",
    "\n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                \n",
    "                # --- METRICS CALCULATION ---\n",
    "                acc = accuracy_score(all_targets, all_preds)\n",
    "                b_acc = balanced_accuracy_score(all_targets, all_preds)\n",
    "                f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "                \n",
    "                scheduler.step(avg_val_loss)\n",
    "\n",
    "                # Snapshot all metrics if Balanced Accuracy improves\n",
    "                if b_acc > best_fold_bacc:\n",
    "                    best_fold_bacc = b_acc\n",
    "                    best_fold_acc = acc\n",
    "                    best_fold_f1 = f1\n",
    "\n",
    "            end_time = time.time()\n",
    "            fold_times.append(end_time - start_time)\n",
    "\n",
    "            # End of Fold: Record the SNAPSHOT metrics\n",
    "            fold_baccs.append(best_fold_bacc)\n",
    "            fold_accs.append(best_fold_acc)\n",
    "            fold_f1s.append(best_fold_f1)\n",
    "            \n",
    "            print(f\" Fold {fold+1} Best: B-Acc: {best_fold_bacc:.4f} | Acc: {best_fold_acc:.4f} | F1: {best_fold_f1:.4f}\")\n",
    "\n",
    "        # Average performance across the 3 folds\n",
    "        mean_bacc = np.mean(fold_baccs)\n",
    "        mean_acc = np.mean(fold_accs)\n",
    "        mean_f1 = np.mean(fold_f1s)\n",
    "        mean_time = np.mean(fold_times)\n",
    "        \n",
    "        full_params = {**params}\n",
    "        \n",
    "        # Store results in a dictionary structure similar to sklearn cv_results_\n",
    "        results.append({\n",
    "            'params': full_params, \n",
    "            'mean_test_balanced_acc': mean_bacc,\n",
    "            'mean_test_acc': mean_acc,\n",
    "            'mean_test_f1_weighted': mean_f1,\n",
    "            'mean_fit_time': mean_time,\n",
    "            'config_id': i \n",
    "        })\n",
    "        \n",
    "        print(f\"Combo {i} Result: Mean B-Acc: {mean_bacc:.4f}\\n\")\n",
    "\n",
    "    # --- Create DataFrame for Reporting ---\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results.sort_values(by='mean_test_balanced_acc', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL GRID SEARCH RESULTS\")\n",
    "    print(\"=\"*40)\n",
    "    print(df_results[['config_id', 'mean_test_acc', 'mean_test_balanced_acc', 'mean_test_f1_weighted']])\n",
    "    \n",
    "    best_result = df_results.iloc[0]\n",
    "    print(f\"\\nBest Hyperparams found (ID {best_result['config_id']}): {best_result['params']}\")\n",
    "    \n",
    "    return best_result['params'], df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38c3d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(self, cont_dim, hidden_dims=[128, 64, 32], output_dim=7):\n",
    "        super().__init__()\n",
    "        zip_embed_dim = 64\n",
    "        num_zip_codes = 883 # borrower_address_zip 882 different values + 1 missing\n",
    "        self.emb = nn.Embedding(num_zip_codes, zip_embed_dim)\n",
    "        \n",
    "        self.input_dim = cont_dim + zip_embed_dim\n",
    "\n",
    "        layers = []\n",
    "        in_dim = self.input_dim\n",
    "        \n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            in_dim = h_dim\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        # Projects the last hidden layer to the number of classes (7)\n",
    "        self.head = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "    def forward(self, X_cont, X_zip):\n",
    "        # Embed zip codes\n",
    "        # Result shape: (Batch_Size, zip_embed_dim)\n",
    "        zip_embedded = self.emb(X_zip)\n",
    "        \n",
    "        # Concatenate continuous features + embeddings\n",
    "        # Result shape: (Batch_Size, cont_dim + zip_embed_dim)\n",
    "        x = torch.cat([X_cont, zip_embedded], dim=1)\n",
    "        \n",
    "        # Pass through MLP features\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # Final classification\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75e8a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search | 3 Combos | 3-Fold CV\n",
      "Fitting Combo 1/3: {'hidden_dims': [128, 64, 32]}\n",
      " Fold 1 Best: B-Acc: 0.8586 | Acc: 0.8970 | F1: 0.8969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:261: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 2 Best: B-Acc: 0.8590 | Acc: 0.8991 | F1: 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:261: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 3 Best: B-Acc: 0.8562 | Acc: 0.8917 | F1: 0.8916\n",
      "Combo 0 Result: Mean B-Acc: 0.8579\n",
      "\n",
      "Fitting Combo 2/3: {'hidden_dims': [256, 128, 64, 32]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:261: UserWarning: Found unknown categories in columns [0, 2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 1 Best: B-Acc: 0.8559 | Acc: 0.8942 | F1: 0.8941\n",
      " Fold 2 Best: B-Acc: 0.8622 | Acc: 0.8943 | F1: 0.8941\n",
      " Fold 3 Best: B-Acc: 0.8617 | Acc: 0.8958 | F1: 0.8957\n",
      "Combo 1 Result: Mean B-Acc: 0.8599\n",
      "\n",
      "Fitting Combo 3/3: {'hidden_dims': [64, 32]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:261: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 1 Best: B-Acc: 0.8452 | Acc: 0.8885 | F1: 0.8885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geko/unibo/data_analytics/project/.venv/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:261: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 2 Best: B-Acc: 0.8441 | Acc: 0.8896 | F1: 0.8898\n",
      " Fold 3 Best: B-Acc: 0.8397 | Acc: 0.8854 | F1: 0.8855\n",
      "Combo 2 Result: Mean B-Acc: 0.8430\n",
      "\n",
      "\n",
      "========================================\n",
      "FINAL GRID SEARCH RESULTS\n",
      "========================================\n",
      "   config_id  mean_test_acc  mean_test_balanced_acc  mean_test_f1_weighted\n",
      "1          1       0.894757                0.859939               0.894614\n",
      "0          0       0.895937                0.857934               0.895923\n",
      "2          2       0.887837                0.842988               0.887932\n",
      "\n",
      "Best Hyperparams found (ID 1): {'hidden_dims': [256, 128, 64, 32]}\n",
      "Finished GridSearchCV, best parameters:\n",
      "{'hidden_dims': [256, 128, 64, 32]}\n",
      "---\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "params & mean_test_balanced_acc & mean_test_acc & mean_test_f1_weighted & mean_fit_time & config_id \\\\\n",
      "\\midrule\n",
      "{'hidden_dims': [256, 128, 64, 32]} & 0.859939 & 0.894757 & 0.894614 & 258.120205 & 1 \\\\\n",
      "{'hidden_dims': [128, 64, 32]} & 0.857934 & 0.895937 & 0.895923 & 237.054176 & 0 \\\\\n",
      "{'hidden_dims': [64, 32]} & 0.842988 & 0.887837 & 0.887932 & 222.662287 & 2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the number of numerical features of the model is defined after training\n",
    "# we need to pass it as a network parameter\n",
    "input_cont_dim = X_train.shape[1]\n",
    "\n",
    "param_grid = {\n",
    "    # 'cont_dim': [input_cont_dim],\n",
    "    'hidden_dims': [\n",
    "        [128, 64, 32],      # Original\n",
    "        [256, 128, 64, 32], # Deeper/Wider\n",
    "        [64, 32]            # Shallower (prevent overfitting)\n",
    "    ],\n",
    "}\n",
    "\n",
    "best_params, df_results = run_cv_grid_search(\n",
    "    model_class=FeedForwardModel, \n",
    "    param_grid=param_grid, \n",
    "    X_raw=X_train, \n",
    "    y_raw=y_train, \n",
    "    n_splits=3\n",
    ")\n",
    "\n",
    "print(\"Finished GridSearchCV, best parameters:\")\n",
    "print(best_params)\n",
    "print(\"---\")\n",
    "print(df_results.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
