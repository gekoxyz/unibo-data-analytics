\documentclass{article}
\usepackage[english]{babel}
\usepackage[a4paper,top=2.5cm,bottom=3cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}

% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},   
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\ttfamily\footnotesize,
%     breakatwhitespace=false,         
%     breaklines=true,                 
%     captionpos=b,                    
%     keepspaces=true,                 
%     numbers=left,                    
%     numbersep=5pt,                  
%     showspaces=false,                
%     showstringspaces=false,
%     showtabs=false,                  
%     tabsize=2
% }

\usepackage{sourcecodepro}

% Nord / Minimal palette
\definecolor{bglight}{HTML}{f8f9fa}     % Warm white/off-white
\definecolor{fgdark}{HTML}{2e3440}      % Dark slate text
\definecolor{keywordblue}{HTML}{5e81ac} % Muted blue
\definecolor{stringteal}{HTML}{a3be8c}  % Soft teal
\definecolor{commentgreen}{HTML}{8fbcbb}% Seafoam comments
\definecolor{bordergray}{HTML}{e5e9f0}  % Very light border

\lstdefinestyle{modern-light}{
    backgroundcolor=\color{bglight},
    basicstyle=\ttfamily\small\color{fgdark},
    commentstyle=\color{commentgreen}\itshape\small,
    keywordstyle=\color{keywordblue}\bfseries,
    stringstyle=\color{stringteal},
    numberstyle=\tiny\color{gray},
    breaklines=true,
    captionpos=b,
    numbers=left,
    numbersep=12pt,
    xleftmargin=3em,
    framexleftmargin=2.5em,
    frame=single,
    rulecolor=\color{bordergray},
    rulesepcolor=\color{bordergray},
    aboveskip=1.5em,
    belowskip=1.5em,
    showstringspaces=false,
    tabsize=2,
    columns=flexible,
    keepspaces=true
}

\lstset{style=modern-light}

\graphicspath{ {./images/} }

\title{Data Analytics}

\author{
 Matteo Galiazzo \\
  Dipartimento di Informatica - Scienza e Ingegneria\\
  Universit√† di Bologna\\
  \texttt{matteo.galiazzo@studio.unibo.it} \\
}

\begin{document}

\maketitle

\begin{abstract}
Your abstract text goes here.
\end{abstract}

\tableofcontents

\section{Data acquisition and visualization}
The raw dataset consists of \textbf{148301} samples and \textbf{145} features. The target variable is \verb|grade|,
which represents the risk classification associated with the borrower.

\subsection{Target distribution}

The distribution of the target variable is visualized in Figure \ref{fig:risk_grades}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{./images/grades_distribution.png}
  \caption{Distribution of borrower risk grades}
  \label{fig:risk_grades}
\end{figure}

\subsection{Feature Correlations}

To identify driving factors, the correlation between the loan \verb|grade| and all numerical features was calculated.
The top correlations are displayed in Figure \ref{fig:best_correlations}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{./images/best_correlations.png}
  \caption{Top 5 positive and negative correlations with the target variable.}
  \label{fig:best_correlations}
\end{figure}

The feature with the highest absolute correlation is \verb|loan_contract_interest_rate|. This confirms the domain
expectation that higher interest rates are assigned to riskier borrower grades. The box plot in Figure
\ref{fig:grade_vs_interestrate} illustrates the clear separation of interest rates across different grade levels.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{./images/grade_vs_interestrate.png}
  \caption{Distribution of interest rates stratified by loan grade.}
  \label{fig:grade_vs_interestrate}
\end{figure}

\section{Preprocessing}

\subsection{Handling Missing Values}

An analysis of missing data revealed significant sparsity in several columns (see Figure
\ref{fig:missing_features_heatmap}).

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{./images/missing_features_heatmap.png}
  \caption{Heatmap of the missing values for the features.}
  \label{fig:missing_features_heatmap}
\end{figure}

To preserve data quality and avoid noise from excessive imputation, features with more than \textbf{20\% missing values}
were discarded (Figure \ref{fig:missing_more_than_20}). Following this reduction, the dataset dimensionality decreased
from 145 to 88 columns.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{./images/missing_more_than_20.png}
  \caption{Features exceeding the 20\% missing value threshold.}
  \label{fig:missing_more_than_20}
\end{figure}

\subsection{Feature Engineering and Encoding}

Categorical features were identified using the Pandas \verb|select_dtypes| method. The identified features include:

\begin{lstlisting}[language=python]
categorical_cols = dataset.select_dtypes(include=['object', 'category']).columns
print(f"Categorical columns:\n{categorical_cols.sort_values()}") 
\end{lstlisting}
\begin{lstlisting}
Categorical columns:
Index(['application_type_label', 'borrower_address_state',
        'borrower_address_zip', 'borrower_housing_ownership_status',
        'borrower_income_verification_status',
        'borrower_profile_employment_length', 'credit_history_earliest_line',
        'debt_settlement_flag_indicator', 'disbursement_method_type', 'grade',
        'hardship_flag_indicator', 'last_credit_pull_date', 'last_payment_date',
        'listing_initial_status', 'loan_contract_term_months',
        'loan_issue_date', 'loan_payment_plan_flag', 'loan_purpose_category',
        'loan_status_current_code'],
      dtype='object')
\end{lstlisting}

\subsubsection{Custom Transformation Pipelines}

To ensure reproducibility and prevent data leakage, distinct preprocessing pipelines were constructed for different
data types:

\begin{itemize}
  \item \textbf{Numeric Extraction}: A regex-based \verb|NumericExtractor| was implemented to parse integers from
  mixed-string columns.
  \item \textbf{Cyclical Date Encoding}: Dates were handled by the \verb|CyclicalDateEncoder|. The year was extracted
  as an ordinal feature, while months were transformed into sine and cosine components. This preserves the cyclical
  temporal proximity between December and January.
  \item \textbf{Binary Encoding}: A \verb|BinaryModeEncoder| was created for binary categorical variables, assigning
  the positive class (1) to the minority value to capture information density.
\end{itemize}

For categorical columns I used sklearn's \verb|OneHotEncoder|.

\section{Machine Learning models}

\subsection{Random Forest classifier}

The Random Forest model was trained using the following preprocessing steps:
\begin{itemize}
  \item \textbf{Missing value thresholding}: Columns with more than 20\% missing values were dropped using
  \texttt{HighMissingDropper(threshold=0.2)}
  \item \textbf{Feature encoding}: Categorical variables were processed using:
  \begin{itemize}
    \item \texttt{NumericExtractor} for parsing mixed-type numeric fields.
    \item \texttt{OneHotEncoder} (with \texttt{handle\_unknown='ignore'} for robustness on unseen categories).
    \item \texttt{CyclicalDateEncoder} to transform temporal features into sine/cosine pairs, preserving cyclical
    relationships (e.g., month 12 is closer to 1 than to 6).
    \item \texttt{BinaryModeEncoder} for binary categorical features.
  \end{itemize}
  \item \textbf{Dimensionality reduction}: High-cardinality nominal categorical features (\texttt{borrower\_address\_zip} and
  \texttt{borrower\_address\_state}) were excluded from One-Hot Encoding to prevent feature space explosion, as
  Random Forest efficiency degrades with high-dimensional sparse inputs despite its robustness to feature dimensionality
  in general.
  \item \textbf{Redundant feature removal}: The \texttt{loan\_title} column was dropped due to redundancy with
  \texttt{loan\_purpose\_category}
\end{itemize}

Missing values were not imputed, as Random Forests handle them natively during tree construction.
Variables were not scaled since Random Forests make decision based on higher/lower and cannot be influenced by feature
size.

A grid search was performed to optimize the following hyperparameters:
\begin{itemize}
  \item \texttt{max\_depth}: \{20, None\}: to limit tree depth to prevent overfitting.
  \item \texttt{n\_estimators}: \{100, 200\}: to balance computational cost with ensemble diversity.
  \item \texttt{min\_samples\_leaf}: \{1, 5\}: to control leaf granularity and decision boundaries smoothness.
\end{itemize}

The model uses \texttt{class\_weight='balanced'} to account for class imbalance. Model selection was based on
\textbf{maximizing balanced accuracy} via stratified $k$-fold cross-validation to ensure robustness across data splits.

\begin{lstlisting}[language=python, basicstyle=\footnotesize\ttfamily, breaklines=true, frame=single, caption={Random Forest Pipeline}]
preprocessor = ColumnTransformer(
    transformers=[
        ('extract', NumericExtractor(), extract_fields),
        ('categorical', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), one_hot_encoding_cols),
        ('date', CyclicalDateEncoder(), date_fields),
        ('binary', BinaryModeEncoder(), binary_cols),
        ('drop_redundant', 'drop', redundant_cols)
    ], 
    remainder="passthrough"
)

full_pipeline = Pipeline([
    ('dropper', HighMissingDropper(threshold=20)),
    ('prep', preprocessor),
    ('rf', RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=42))
])

param_grid = {
    'rf__max_depth': [20, None],
    'rf__n_estimators': [100, 200],
    'rf__min_samples_leaf': [1, 5]
}
\end{lstlisting}


\subsection{K-Nearest Neighbours}

The $K$-Nearest Neighbours model was trained using the following preprocessing steps:

\begin{itemize}
  \item \textbf{Missing value thresholding}: Columns with more than 20\% missing values were dropped using
  \texttt{HighMissingDropper(threshold=0.2)}
  \item \textbf{Feature encoding}: Categorical variables were processed using:
  \begin{itemize}
    \item \texttt{NumericExtractor} for parsing mixed-type numeric fields, followed by median imputing and standard scaling
    \item \texttt{OneHotEncoder} for nominal categorical columns. Before
    this I did constant imputation by filling missing values with the string "missing".
    \item \texttt{CyclicalDateEncoder} to transform temporal categorical features into sine/cosine pairs, preserving cyclical
    relationships (e.g. month 12 is closer to 1 than to 6). Then I applied median imputation for missing values,
    and standard scaling which is not needed for sine/cosine but for the year column.
    \item \texttt{BinaryModeEncoder} for binary categorical features, with imputation with the most frequent class
    after that.
  \end{itemize}
  \item \textbf{Redundant feature removal}: The \texttt{loan\_title} column was dropped due to redundancy with
  \texttt{loan\_purpose\_category}
  \item \textbf{Numerical imputation}: The remaining numerical columns were treated with median imputing and standard scaling,
  because KNN relies on distance metrics, scaling is mandatory to prevent features with larger ranges from dominating the
  distance calculation.
\end{itemize}

A grid search was performed to optimize the following hyperparameters:
\begin{itemize}
  \item \texttt{n\_neighbors}: \{3, 5, 7\}: to balance computational cost with decision boundaries smoothness.
  \item \texttt{LDA}: \{LDA(), passthrough\}: to check if projecting the data into a lower dimension space that maximizes
  class separability helps the model.
  \item \texttt{Sampling}: \{passthrough, randomUnderSampler, SMOTE\}: to check which strategy handles the class imbalance
  in a better way. Random undersampling reduces the majority class, while SMOTE (Synthetic Minority OverSampling Technique)
  generates synthetic examples to bolster the minority class representation.
\end{itemize}

The model uses \texttt{weights='distance'} to weight points by the inverse of their distance, making closer points have
a greater influence in the voting. This mitigates the presence of the outliers in the neighbor points.

Model selection was based on \textbf{maximizing balanced accuracy} via stratified $k$-fold cross-validation to ensure
robustness across data splits.

\begin{lstlisting}[language=python, basicstyle=\footnotesize\ttfamily, breaklines=true, frame=single, caption={Random Forest Pipeline}]
  numeric_pipe = SkPipeline([
    ('extract', NumericExtractor()),
    ('impute', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_pipe = SkPipeline([
    ('impute', SimpleImputer(strategy='constant', fill_value='missing')), # Prevent OHE crash
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
])

date_pipe = SkPipeline([
    ('cyclical', CyclicalDateEncoder()),
    ('impute', SimpleImputer(strategy='median')), # Imputing sin/cos is mathematically valid
    ('scaler', StandardScaler())
])

binary_pipe = SkPipeline([
    ('binary_enc', BinaryModeEncoder()), 
    ('impute', SimpleImputer(strategy='most_frequent')) # Safety net
])

# remainder columns are numerical
remainder_pipe = SkPipeline([
    ('impute', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num_pipe', numeric_pipe, extract_fields),
        ('cat_pipe', categorical_pipe, one_hot_encoding_cols),
        ('date_pipe', date_pipe, date_fields),
        ('bin_pipe', binary_pipe, binary_cols),
        ('drop_redundant', 'drop', redundant_cols)
    ], 
    remainder=remainder_pipe
)

full_pipeline = ImbPipeline([
    ('dropper', HighMissingDropper(threshold=20)),
    ('prep', preprocessor),
    ('sampler', 'passthrough'),
    ('lda', LDA()),
    ('knn', KNeighborsClassifier(weights='distance'))
])

param_grid = {
    'knn__n_neighbors': [3, 5, 7],
    'lda': ['passthrough', LDA()],
    'sampler': ['passthrough', RandomUnderSampler(random_state=SEED), SMOTE(random_state=SEED)]
}

scoring = {
    'acc': 'accuracy',
    'balanced_acc': 'balanced_accuracy',
    'f1_weighted': 'f1_weighted'
}

grid_search = GridSearchCV(
    estimator=full_pipeline,
    param_grid=param_grid,
    cv=3,
    scoring=scoring,
    refit='balanced_acc',
    n_jobs=-1,
    verbose=1
)
\end{lstlisting}


\subsection{Support Vector Machine}

The Support Vector Machine was trained using the following preprocessing steps:
\begin{itemize}
  \item \textbf{Missing value thresholding}: Columns with more than 20\% missing values were dropped using
  \texttt{HighMissingDropper(threshold=0.2)}
  \item \textbf{Feature encoding}: Categorical variables were processed using:
  \begin{itemize}
    \item \texttt{NumericExtractor} for parsing mixed-type numeric fields, followed by median imputing and standard scaling
    \item \texttt{OneHotEncoder} for nominal categorical columns. Before
    this I did constant imputation by filling missing values with the string "missing".
    \item \texttt{CyclicalDateEncoder} to transform temporal categorical features into sine/cosine pairs, preserving cyclical
    relationships (e.g. month 12 is closer to 1 than to 6). Then I applied median imputation for missing values,
    and standard scaling which is not needed for sine/cosine but for the year column.
    \item \texttt{BinaryModeEncoder} for binary categorical features, with imputation with the most frequent class
    after that.
  \end{itemize}
  \item \textbf{Redundant feature removal}: The \texttt{loan\_title} column was dropped due to redundancy with
  \texttt{loan\_purpose\_category}
  \item \textbf{Numerical imputation}: The remaining numerical columns were treated with median imputing and standard scaling,
  as SVMs are sensitive to the scale of input features due to their reliance on distance-based calculations.
\end{itemize}

A grid search was performed to optimize the following hyperparameters:
\begin{itemize}
  \item \texttt{max\_iter}: \{1000, 10000\}: to ensure the algorithm reaches convergence.
  \item \texttt{C}: \{0.01, 0.1, 1, 10, 100\}: to control the bias-variance tradeoff; smaller values of C create a wider margin
  (more regularization), while larger values penalize misclassifications more heavily, potentially leading to overfitting.
  \item \texttt{LDA}: \{'passthrough', LDA()\}: to check if dimensionality reduction via Linear Discriminant Analysis improves
  performance by projecting the data into a subspace that maximizes class separability.
\end{itemize}

The model uses \texttt{class_weights='balanced'} to to account for class imbalance.
Model selection was based on \textbf{maximizing balanced accuracy} via stratified $k$-fold cross-validation to ensure
robustness across data splits.

\begin{lstlisting}[language=python, basicstyle=\footnotesize\ttfamily, breaklines=true, frame=single, caption={Random Forest Pipeline}]
  numeric_pipe = SkPipeline([
      ('extract', NumericExtractor()),
      ('impute', SimpleImputer(strategy='median')),
      ('scaler', StandardScaler())
  ])
  
  categorical_pipe = SkPipeline([
      ('impute', SimpleImputer(strategy='constant', fill_value='missing')),
      ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
  ])
  
  date_pipe = SkPipeline([
      ('cyclical', CyclicalDateEncoder()),
      ('impute', SimpleImputer(strategy='median')),
      ('scaler', StandardScaler())
  ])
  
  binary_pipe = SkPipeline([
      ('binary_enc', BinaryModeEncoder()), 
      ('impute', SimpleImputer(strategy='most_frequent'))
  ])
  
  # remainder columns are numerical
  remainder_pipe = SkPipeline([
      ('impute', SimpleImputer(strategy='median')),
      ('scaler', StandardScaler())
  ])
  
  preprocessor = ColumnTransformer(
      transformers=[
          ('num_pipe', numeric_pipe, extract_fields),
          ('cat_pipe', categorical_pipe, one_hot_encoding_cols),
          ('date_pipe', date_pipe, date_fields),
          ('bin_pipe', binary_pipe, binary_cols),
          ('drop_redundant', 'drop', redundant_cols)
      ], 
      remainder=remainder_pipe 
  )
  
  full_pipeline = SkPipeline([
      ('dropper', HighMissingDropper(threshold=20)),
      ('prep', preprocessor),
      ('lda', LDA()),
      ('svc', SVC(kernel="rbf", class_weight="balanced", C=1.0, max_iter=10000))
  ])
  
  param_grid = {
      'svc__max_iter': [1000, 10000],
      'svc__C': [0.01, 0.1, 1, 10, 100],
      'lda': ['passthrough', LDA()]
  }
  
  scoring = {
      'acc': 'accuracy',
      'balanced_acc': 'balanced_accuracy',
      'f1_weighted': 'f1_weighted'
  }
  
  grid_search = GridSearchCV(
      estimator=full_pipeline,
      param_grid=param_grid,
      cv=5,
      scoring=scoring,
      refit='balanced_acc',
      n_jobs=-1,
      verbose=1
  )
\end{lstlisting}


\section{Deep Learning models}

\subsection{FeedForward network}

The FeedForward network was trained using the following preprocessing steps:
\begin{itemize}
  \item \textbf{Missing value thresholding}: Columns with more than 20\% missing values were dropped using
  \texttt{HighMissingDropper(threshold=0.2)}
  \item \textbf{Feature encoding}: Categorical variables were processed using:
  \begin{itemize}
    \item \texttt{NumericExtractor} for parsing mixed-type numeric fields, followed by median imputing and standard scaling
    \item \texttt{OneHotEncoder} for nominal categorical columns. Before
    this I did constant imputation by filling missing values with the string "missing".
    \item \texttt{CyclicalDateEncoder} to transform temporal categorical features into sine/cosine pairs, preserving cyclical
    relationships (e.g. month 12 is closer to 1 than to 6). Then I applied median imputation for missing values,
    and standard scaling which is not needed for sine/cosine but for the year column.
    \item \texttt{BinaryModeEncoder} for binary categorical features, with imputation with the most frequent class
    after that.
  \end{itemize}
  \item \textbf{Redundant feature removal}: The \texttt{loan\_title} column was dropped due to redundancy with
  \texttt{loan\_purpose\_category}
  \item \textbf{Numerical imputation}: The remaining numerical columns were treated with median imputing and standard scaling,
  since neural networks are sensitive to the scale of input features.
  \item \textbf{Zip code encoding}: The \texttt{zip\_pipeline} column was imputed by filling the missing values with the string
  "missing" and by ordinal encoding, to prepare the column to be embedded by the neural network.
\end{itemize}

\paragraph{Network architecture}
The network is a FeedForward network that embeds the zip code, concatenates it with the numerical features and then
for each layer of the network applies:
\begin{itemize}
  \item Linear layer: to perform affine transformations and learn complex linear combinations of the input features.
  \item Batch Normalization: to stabilize and accelerate training by re-centering and re-scaling the inputs to each
  layer, which mitigates the "internal covariate shift".
  \item Leaky ReLU activation: to introduce non-linearity while preventing the "dying ReLU" problem by allowing a small,
  non-zero gradient for negative input values.
  \item Dropout with probability 0.2 as a regularization technique to prevent overfitting by randomly zeroing 20\% of the
  neurons during training, forcing the network to learn redundant representations.
\end{itemize}

\paragraph{Training and hyperparameters}
The fixed hyperparameters are:
\begin{itemize}
  \item \texttt{CrossEntropyLoss}, with class weights: to penalize misclassifications of the minority class more heavily,
        directly adressing the dataset's class imbalance.
  \item \texttt{AdamW} optimizer with a learning rate of $\mu = 1*10^{-3}$: chosen for the adaptive learning rate and
        weight decay, to provide more stable gradient estimates.
  \item \texttt{ReduceLROnPlateau}: scheduler (with a minimum of $\mu = 1*10^{-6}$) to reduce the learning rate when the
        validation loss plateaus, to make the optimizer converge more precisely on the minimum.
  \item \textbf{Epochs and Batch size}: the model was trained for 200 epochs with a batch size of 1024.
\end{itemize}

A grid search was performed to optimize the \texttt{hidden\_dims} hyperparameter, the evaluated dimensions were:
\{[128, 64, 32], [256, 128, 64, 32], [64, 32]\}. This determines the optimal balance between bias and variance.

\begin{lstlisting}[language=python, basicstyle=\footnotesize\ttfamily, breaklines=true, frame=single, caption={Random Forest Pipeline}]
\end{lstlisting}


\section{Deep Tabular models}

\subsection{TabNet}


\subsection{TabTransformer}


\end{document}